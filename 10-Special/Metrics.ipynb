{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Метрики в рекомендательных системах"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В классической задаче рекомендательной системы имеем данные:\n",
    "* Пользователей $u\\in {Users}, N = |Users|$\n",
    "* Товары $i\\in {Items}, M = |Items|$\n",
    "* Рейтинг $r_{ui}\\in {Ratings} $\n",
    "* Prediction $p_{ui}\\in {Prediction} $\n",
    "\n",
    "Матрица оценок user-item является сильно разряженной и нам требуется заполнить пропущенные значения."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для того чтобы оценить рекомендательную систему есть несколько способов и метрик, которые нужно использовать в зависимости от задачи:\n",
    "* Error metrics\n",
    "* Desicion-support metrics\n",
    "* User and Usage-centered metrics\n",
    "\n",
    "Для оценки модели оставляют отложенную выборку <b>по времени</b> (в выборке для обучения данные, которые предшествуют отложенной выборке). Также хорошим правилом будет провести оценку кросс-валидацией по <b>пользователям</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Error metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Error metrics</b> расчитываются как разность предсказанным значением и оценкой пользователя на тестовой выборке. К таким метрикам относятся:\n",
    "$$ MAE = \\frac{\\sum_{r \\in Ratings, p \\in Predictions}|p - r|}{|Ratings|} $$\n",
    "$$ MSE = \\frac{\\sum_{r \\in Ratings, p \\in Predictions}(p - r)^2}{|Ratings|} $$\n",
    "$$ RMSE = \\sqrt{\\frac{\\sum_{r \\in Ratings, p \\in Predictions}(p - r)^2}{|Ratings|}} $$\n",
    "Ореинтируясь на эти метрики мы пытаемся как можно лучше приблизить рейтинг выданной моделью к рейтингу пользователя. Ими можно ореинтироваться при обучении модели.\n",
    "Лучше использовать MSE и RMSE, так как они дают больший штраф при большей разнице между предсказаниями, а RMSE дает интуитивно понятную величину оценки модели."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision-support metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Однако, при оценке рекомендательной системы нас не должно волновать какую оценку она поставит данному продукту 3.7 или 3.2. Нам важно, чтобы рекомендательная система научилась оценивать выше хорошие или подходящие товары чем товары, которые будут не интересны. Этот вид метрик работает с бинарными значениями - 0 и 1 (просмотр, лайк, рейтинг >= 3.5 итд). А получив список рекомендаций мы хотим видеть самые подходящие в верху списка. Поэтому Decision-support метрики чаще рассматривают на <b>топ k рекомендациях</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Метрика **Precision** показывает долю товаров рекомендаций понравившихся пользователю, precision@k считается на топ k рекомендациях\n",
    "$$Precision@k = \\frac{1}{K}{\\sum_{k=1}^{K} Relevance(k)}$$, где $Relevance(k)$ - индикатор релевантности i товара. <br>\n",
    "Метрика **Recall** показывает долю рекомендаций показанных пользователю, recall@k считается на топ k рекомендациях\n",
    "$$Recall@k = \\frac{\\sum_{k=1}^{K} Relevance(k)}{|Relevant|}$$, где # relevant - кол-во понравившихся товаров, $Relevance(k)$ - индикатор релевантности i товара."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Метрика **Average Precision@k** - в отличие от Precision@k и Recall@k, AP@k придает значение не только списку, но и самому порядку этих рекомендаций\n",
    "$$AP@K = \\frac{\\sum_{k=1}^{K}(Relevance(k)\\cdot Precision@k))}{\\sum_{k=1}^{K}Relevance(k)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Метрика **Mean Average Precision @ k (MAP@K)** - наиболее часто использующаяся метрика качества в задачах рекомендательных систем, она получается подсчетом среднего значения по всем пользователям по AP@k\n",
    "$$ MAP@K = \\frac{1}{N}\\sum_{u\\in {Users}}{AP@K(u)} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Метрики придающие значению на ранжированию внутри топа"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Метрика **Mean Reciprocal Rank** (MRR) - для каждого пользователя берется обратная дробь порядка первого товара, который понравился пользователю, в списке рекомендаций модели.\n",
    "$$MRR@K = \\frac{1}{N}\\sum_{i=1}^{N}\\frac{1}{RR@k}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Метрика **Comulative Gain (CG@k)** - простое кол-во релевантных элементов в топе, чем больше - тем лучше. **Не учитывает порядок**\n",
    "$$ CG@K = \\sum_{k=1}^{K}Relevance(k) * Rating(k)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Метрика **Discounted Cumulative Gain (DCG@k)**, в отличии от предыдущих делает дисконтирование с каждой позицией в списке, делая акцент на то, чтобы наиболее релевантные продукты шли первыми по списку\n",
    "$$ DCG@K = \\sum_{k=1}^{K}\\frac{Relevance(k)Rating(k)}{log_2(k+1)} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Метрика **Normalised Discounted Comulative Gain** (NDCG@K) - метрика качества, нормализует DCG@K по IDCG@K наилучшему значению DCG@k.\n",
    "$$IDCG@K = \\sum_{k=1}^{K}\\frac{1}{log_2(k+1)}$$\n",
    "$$NDCG@K = \\frac{DCG@K}{NDCG@K}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## User and Usage-centered metrics "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mae():\n",
    "    pass\n",
    "\n",
    "def mse():\n",
    "    pass\n",
    "\n",
    "def rmse():\n",
    "    pass\n",
    "\n",
    "def get_rr():\n",
    "    pass\n",
    "\n",
    "def get_mrr(predicted, target):\n",
    "    # write your code here\n",
    "    pass\n",
    "\n",
    "def get_precision_at_k():\n",
    "    # write your code here\n",
    "    pass\n",
    "\n",
    "def get_recall_at_k():\n",
    "    # write your code here\n",
    "    pass\n",
    "\n",
    "def get_map_at_k():\n",
    "    # write your code here\n",
    "    pass\n",
    "\n",
    "def get_cg():\n",
    "    # write your code here\n",
    "    pass\n",
    "\n",
    "def get_dcg():\n",
    "    # write your code here\n",
    "    pass\n",
    "\n",
    "def get_ndcg():\n",
    "    # write your code here\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_list = [\n",
    "    [0, 1, 2, 3, 4],              \n",
    "    [4, 5, 6, 7, 8]\n",
    "]\n",
    "# думайте о числах, как об id продуктов\n",
    "target_list = [\n",
    "    [0, 2, 4, 5, 6],\n",
    "    [1, 3, 6, 7, 0]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-fba5e98a06fa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32massert\u001b[0m \u001b[0mget_mrr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "assert np.round(get_mrr(predicted_list, target_list), 2) == 0.67"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_list = [0, 1, 2, 4, 8]              \n",
    "target_list = [0, 2, 4, 5, 6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert get_precision_at_k(predicted_list, target_list, k=3) == 0.5\n",
    "assert get_precision_at_k(predicted_list, target_list, k=5) == 0.5 \n",
    "assert get_precision_at_k([1, 2, 3, 4], [5, 6, 7, 5], 3) == 0 # нет полезных рекомендаций"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert np.round(get_recall_at_l(predicted_list, target_list, k=3)) == 0.67\n",
    "assert get_recall_at_l(predicted_list, target_list, k=3) == 0.67"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
