{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Регуляризация xgboost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Более подробно"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Данный регуляризатор подобран эвристически и хорошо показывает себя на практике. \n",
    "\n",
    "Раскладывая в ряд Тейлора выражение $\\mathcal{L}\\left(y_i, \\hat{y}_i^{\\left(t-1\\right)} + b_t(x_i)\\right)$ до второго порядка, получаем:\n",
    "\n",
    "$$ Obj^{(t)} = \\sum_{i=1}^N\\left[\\mathcal{L}(y_i, \\hat{y}_i^{\\left(t-1\\right)}) + g_{i}b_{t}(x_i) + \\frac{1}{2}h_{i}b_{t}^2(x_i)\\right] + \\Omega(b_t),$$\n",
    "\n",
    "где $g_i = \\partial_{\\hat{y}_i^{(t-1)}} \\mathcal{L}(y_i, \\hat{y_i}^{(t-1)})$, $h_i = \\partial_{\\hat{y}_i^{(t-1)}}^2 \\mathcal{L}(y_i, \\hat{y}_i^{(t-1)}) $ — градиент и гессиан оптимизируемой функции потерь.\n",
    "\n",
    "Приводя теперь подобные слагаемые и отбрасывая слагаемое $ \\mathcal{L}(y_i, \\hat{y}_i^{(t-1)}) $, не зависящее от $ b_t(x_i)$ (а следовательно, не влияющее на точку минимума функционала), получаем формулу:\n",
    "$$ Obj^{(t)} = \\Big[\\sum_{i=1}^N\\mathcal{L}(y_i, \\hat{y}_i^{\\left(t-1\\right)})\\Big] + \\sum_{i=1}^N\\Big[g_{i}b_{t}(x_i) + \\frac{1}{2}h_{i}b_{t}^2(x_i)\\Big] + \\Big[\\gamma T + \\frac{1}{2}\\lambda\\sum_{j=1}^{T}w_j^2 + \\alpha\\sum_{j=1}^{T}|w_j|\\Big]$$\n",
    "\n",
    "Сделаем хитрый трюк: рассмотрим все объекты, через листья, в которые они попали ($I_j$ - множество объектов, попвших в лист $j$)\n",
    "$$\\sum_{i=1}^{N}g_ib_t(x_i) = \\sum_{j=1}^{T}w_j\\sum_{i\\in I_j}g_i = \\sum_{j=1}^{T}w_jG_j$$\n",
    "$$\\sum_{i=1}^{N}h_ib_t^2(x_i) = \\sum_{j=1}^{T}w^2_j\\sum_{i\\in I_j}h_i = \\sum_{j=1}^{T}w_jH_j$$\n",
    "\n",
    "\n",
    "Тогда получим\n",
    "\n",
    "$$ Obj^{(t)} = \\Big[\\sum_{i=1}^N\\mathcal{L}(y_i, \\hat{y}_i^{\\left(t-1\\right)})\\Big] + \\sum_{j=1}^{T}\\left[G_jw_j + \\frac{1}{2}(H_j + \\lambda)w_j^2 + \\alpha|w_j|\\right] + \\gamma T, $$\n",
    "\n",
    "Заметим, что первое слагаемое не зависит от текущего оптимизируемого дерева $b_t$, значит его можно отбросить и минимизировать оставшийся функционал\n",
    "\n",
    "$$ \\tilde{Obj}^{(t)} = \\sum_{j=1}^{T}\\left[G_jw_j + \\frac{1}{2}(H_j + \\lambda)w_j^2 + \\alpha|w_j|\\right] + \\gamma T \\rightarrow \\min$$\n",
    "\n",
    "\n",
    "Теперь, имея заданную структуру дерева, можно аналитически вычислить оптимальные значения для весов.\n",
    "\n",
    "В общем случае получаем:\n",
    "$$ w_j = \\begin{cases}\n",
    "-\\frac{G_j + \\alpha}{H_j + \\lambda}, если \\space G_j < -\\alpha, \\\\\n",
    "-\\frac{G_j - \\alpha}{H_j + \\lambda}, если \\space G_j > \\alpha, \\\\\n",
    "0, \\space иначе.\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "При $\\alpha = 0$:\n",
    "$$ w_j^* = -\\frac{G_j}{H_j + \\lambda}.$$\n",
    "\n",
    "Значение функционала при $\\alpha = 0$ будет равно:\n",
    "\n",
    "$$ Obj = -\\frac{1}{2}\\sum_{j=1}^T \\frac{G_j^2}{H_j + \\lambda} + \\gamma T .$$\n",
    "\n",
    "Осталось только построить дерево оптимальной структуры. Это можно делать известными методами построения решающих деревьев, проводя разбиения таким образом, чтобы максимизировать gain, определенный как уменьшение $Obj$ в момент этого разбиения. Для уже построенного дерева по формулам $ w_j^* $ вычисляются оптимальные значения в листьях."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
